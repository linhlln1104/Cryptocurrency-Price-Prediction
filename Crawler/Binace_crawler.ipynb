{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "import logging\n",
    "import requests\n",
    "from contextlib import contextmanager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from functools import wraps\n",
    "import pickle\n",
    "\n",
    "def setup_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Nếu logger đã được cấu hình, trả về nó\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "    \n",
    "    # Xóa tất cả handlers hiện có\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Tạo file handler\n",
    "    file_handler = logging.FileHandler('scraper.log')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Tạo console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Tạo formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Thêm handlers vào logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # Ngăn chặn logger truyền messages lên các parent loggers\n",
    "    logger.propagate = False\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Xóa tất cả handlers của root logger\n",
    "logging.getLogger().handlers.clear()\n",
    "# Khởi tạo logger\n",
    "logger = setup_logger()\n",
    "\n",
    "# Rate limiting decorator\n",
    "def rate_limited(max_per_second):\n",
    "    min_interval = 1.0 / float(max_per_second)\n",
    "    def decorate(func):\n",
    "        last_time_called = [0.0]\n",
    "        @wraps(func)\n",
    "        def rate_limited_function(*args, **kwargs):\n",
    "            elapsed = time.time() - last_time_called[0]\n",
    "            left_to_wait = min_interval - elapsed\n",
    "            if left_to_wait > 0:\n",
    "                time.sleep(left_to_wait)\n",
    "            ret = func(*args, **kwargs)\n",
    "            last_time_called[0] = time.time()\n",
    "            return ret\n",
    "        return rate_limited_function\n",
    "    return decorate\n",
    "\n",
    "# Cache functions\n",
    "def load_cache():\n",
    "    try:\n",
    "        with open('scrape_cache.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    with open('scrape_cache.pkl', 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "scrape_cache = load_cache()\n",
    "\n",
    "@contextmanager\n",
    "def managed_chrome_driver(webdriver_path):\n",
    "    \"\"\"\n",
    "    Context manager để quản lý vòng đời của Chrome WebDriver.\n",
    "    \n",
    "    Args:\n",
    "        webdriver_path (str): Đường dẫn đến ChromeDriver.\n",
    "    \n",
    "    Yields:\n",
    "        webdriver.Chrome: Đối tượng WebDriver đã được khởi tạo.\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(service=Service(webdriver_path), options=options)\n",
    "    try:\n",
    "        yield driver\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "@rate_limited(0.1)  # 1 request per 10 seconds\n",
    "def scrape_page(url):\n",
    "    if url in scrape_cache:\n",
    "        logger.info(f\"Using cached data for {url}\")\n",
    "        return scrape_cache[url]\n",
    "\n",
    "    def exponential_backoff(max_retries=5, backoff_factor=2):\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(random.uniform(10, 20) * (backoff_factor ** i))\n",
    "                with requests.Session() as session:\n",
    "                    page = session.get(url)\n",
    "                    page.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                \n",
    "                soup_title = soup.find('h1', class_=\"title\")\n",
    "                title = soup_title.text.strip() if soup_title else 'No Title'\n",
    "                \n",
    "                soup_content = soup.find_all('span', class_=\"richtext-text css-1iqe90x\")\n",
    "                content = ' '.join([c.text.strip() for c in soup_content])\n",
    "                \n",
    "                date_match = re.search(r'/(\\d{4}-\\d{2}-\\d{2})-', url)\n",
    "                date = date_match.group(1) if date_match else 'No Date'\n",
    "                \n",
    "                logger.info(f\"Scraped URL: {url} | Date: {date} | Title: {title[:50]}...\")\n",
    "                \n",
    "                result = (date, title, content)\n",
    "                scrape_cache[url] = result\n",
    "                save_cache(scrape_cache)\n",
    "                return result\n",
    "            except requests.RequestException as e:\n",
    "                if e.response.status_code == 429:\n",
    "                    logger.warning(f\"Rate limit exceeded for {url}. Retrying in {backoff_factor ** i} seconds...\")\n",
    "                    continue\n",
    "                logger.error(f\"Network error while scraping {url}: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error while scraping {url}: {e}\")\n",
    "        logger.error(f\"Failed to scrape {url} after {max_retries} attempts\")\n",
    "        return None, None, None\n",
    "\n",
    "    return exponential_backoff()\n",
    "\n",
    "def get_all_post_urls(base_url, driver, target_date='2020-01-01'):\n",
    "    post_urls = set()\n",
    "    found_target_date = False\n",
    "    scroll_pause_time = random.uniform(15, 30)  # Increased wait time\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    no_new_content_count = 0\n",
    "    max_no_new_content = 10\n",
    "\n",
    "    try:\n",
    "        driver.get(base_url)\n",
    "        time.sleep(random.uniform(10, 15))  # Increased initial wait time\n",
    "\n",
    "        while not found_target_date and no_new_content_count < max_no_new_content:\n",
    "            scroll_height = random.randint(100, 300)  # Reduced scroll amount\n",
    "            driver.execute_script(f\"window.scrollBy(0, {scroll_height});\")\n",
    "            time.sleep(random.uniform(3, 7))  # Increased wait after each scroll\n",
    "\n",
    "            if random.random() < 0.05:  # Reduced frequency to 5%\n",
    "                perform_random_action(driver)\n",
    "\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            if random.random() < 0.2:  # 20% chance to pause for \"reading\"\n",
    "                reading_time = random.uniform(10, 20)  # Increased reading time\n",
    "                logger.info(f\"Pausing to 'read' content for {reading_time:.2f} seconds\")\n",
    "                time.sleep(reading_time)\n",
    "\n",
    "            new_urls_found = find_new_links(driver, post_urls, target_date)\n",
    "            \n",
    "            if new_urls_found:\n",
    "                no_new_content_count = 0\n",
    "            else:\n",
    "                no_new_content_count += 1\n",
    "\n",
    "            if found_target_date:\n",
    "                break\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                for _ in range(3):\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(scroll_pause_time)\n",
    "                    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_height > last_height:\n",
    "                        no_new_content_count = 0\n",
    "                        break\n",
    "                else:\n",
    "                    if no_new_content_count >= max_no_new_content:\n",
    "                        logger.info(\"Reached end of page or no new content. Refreshing...\")\n",
    "                        driver.refresh()\n",
    "                        time.sleep(random.uniform(10, 15))\n",
    "                        no_new_content_count = 0\n",
    "\n",
    "            last_height = new_height\n",
    "            scroll_pause_time = random.uniform(15, 30)  # Increased and randomized pause time\n",
    "\n",
    "            logger.info(f\"URLs found: {len(post_urls)}, Next wait time: {scroll_pause_time:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scrolling: {e}\")\n",
    "\n",
    "    logger.info(f\"Total post URLs collected from {base_url}: {len(post_urls)}\")\n",
    "    return list(post_urls)\n",
    "\n",
    "def perform_random_action(driver):\n",
    "    \"\"\"Thực hiện một hành động ngẫu nhiên trên trang.\"\"\"\n",
    "    try:\n",
    "        actions = [\n",
    "            lambda: driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_UP),\n",
    "            lambda: driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN),\n",
    "            lambda: driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ARROW_UP),\n",
    "            lambda: driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ARROW_DOWN),\n",
    "            lambda: WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.TAG_NAME, 'button'))).click()\n",
    "        ]\n",
    "        random.choice(actions)()\n",
    "        logger.info(\"Performed a random action\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to perform random action: {e}\")\n",
    "\n",
    "def find_new_links(driver, existing_urls, target_date):\n",
    "    \"\"\"Tìm các liên kết mới và thêm vào tập hợp existing_urls.\"\"\"\n",
    "    new_urls_found = False\n",
    "    try:\n",
    "        links = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//a[contains(@href, '/en/square/post/')]\"))\n",
    "        )\n",
    "        for link in links:\n",
    "            url = link.get_attribute('href')\n",
    "            if url not in existing_urls:\n",
    "                date_match = re.search(r'/(\\d{4}-\\d{2}-\\d{2})-', url)\n",
    "                if date_match:\n",
    "                    date = date_match.group(1)\n",
    "                    if date <= target_date:\n",
    "                        return True  # Tìm thấy target date\n",
    "                    existing_urls.add(url)\n",
    "                    new_urls_found = True\n",
    "    except (TimeoutException, StaleElementReferenceException) as e:\n",
    "        logger.warning(f\"Error finding links: {e}\")\n",
    "    return new_urls_found\n",
    "\n",
    "def process_url(url, webdriver_path):\n",
    "    with managed_chrome_driver(webdriver_path) as driver:\n",
    "        try:\n",
    "            post_urls = get_all_post_urls(url, driver, target_date='2020-01-01')\n",
    "            \n",
    "            data = []\n",
    "            for i, post_url in enumerate(post_urls, 1):\n",
    "                date, title, content = scrape_page(post_url)\n",
    "                if date and title and content:\n",
    "                    data.append({'Date': date, 'Title': title, 'Content': content})\n",
    "                \n",
    "                if i % 50 == 0:  # Reduced frequency of temporary saves\n",
    "                    partial_df = pd.DataFrame(data)\n",
    "                    partial_filename = f'temp_{i}_posts.csv'\n",
    "                    partial_df.to_csv(partial_filename, index=False, encoding='utf-8-sig')\n",
    "                    logger.info(f\"Temporary data saved to {partial_filename}\")\n",
    "                \n",
    "                time.sleep(random.uniform(5, 10))  # Added delay between processing URLs\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df = df.sort_values(by='Date', ascending=False).reset_index(drop=True)\n",
    "            \n",
    "            filename = 'bitcoin_news.csv' if 'bitcoin' in url else 'ethereum_news.csv' if 'ethereum' in url else 'output.csv'\n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Data saved to {filename}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Main execution\n",
    "webdriver_path = 'chromedriver.exe'\n",
    "urls = [\n",
    "    'https://www.binance.com/en/square/news/bitcoin%20news',\n",
    "    'https://www.binance.com/en/square/news/ethereum%20news'\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    process_url(url, webdriver_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
